<!DOCTYPE = html>
<html>
  <head>
    <style>
    body{
        background-color: #ffefcc
    }
    </style>
    <title>Alex Fantine's Research</title>
  </head>

  <body>
    <h2> 8/10-8/15- Simulating a General Adversarial Network </h2>

    <p>
      Last time we spoke (or rather I monologued), I was planning on creating a headline classifier, to decide whether headlines are
      real or fake. This in and of itself seems like a pretty simple task, that mostly requires generating a bunch of fake
      headlines and feeding those (along with an equal number of real headlines) as training data into a densely connected model, outputting
      as a single value- 1 if the headline is real, 0 if the headline is "fake" aka generated by my generator. This kind of a model,
      where two neural networks communicate with each other to improve, is known as a General Adversarial Network. My version of one
      of these GANs is far simpler and really only simulates the idea of a real one, but I'm calling it a GAN nonetheless.
    </p>

    <p>
      I started by generating 8600 fake headlines, in sets of ten, with a random starting word from the vocabulary as
      the seed text. I kept the lengths of the fake headlines as only five words (more on why I think this was
      a terrible idea later) and generated them all with a different diversity for each word:
      5 headlines with diversities [1.2, 1.3, 1.4, 1.5, 1.6] and 5 with [2.1, 2.2, 2.3, 2.4, 2.5]. After generating my headlines
      and training a model with those and the real headlines, I had a headline classifier! before training, I split
      my data 90/10 into training/testing data so I could check for validation accuraccy, and that ended up being
      around 97%. This all seemed like fantastic results, so I put my model to work classifying headlines! All is well in
      the world of machine learning!...
    </p>

    <p><img id="meme" alt="Well Yes But" src="./well_yes_but.jpg"></p>

    <p>
      ... Turns out, my old nemesis-overfitting- came back to bite me in the butt. But I didn't realize it at first, because my biggest
      issue was that the model would predict binary values (0 and 1, like I asked it to) rather than percents, so I could see what kind
      of headlines were tripping it up. After a number of other model architectures I tried, I ended up with one that seemed to work
      the way I wanted. I also realized that the only data I care about is the model's false positives, I.E. headlines that were
      fake but it clasified as real. So I had my model show me, <i> from the test data </i>, which headlines were coming up
      as false positives and their probability of being true. And everything seemed to be working, once again, so I planned out my experiment.
      I'm going to:
      <li> Generate 1,000 fake headlines for a set of diversities, starting with [1.0, 1.0, 1.0, 1.0, 1.0] and going through
        <i> literally every single possible combination of diversities </i> until I got to [2.0, 2.0, 2.0, 2.0, 2.0]. </li>
      <li> Feed the fake headlines into the classifier and generate an accuraccy report </li>
      <li> Save the false positive data (% false positives) along with the diversities for each iteration </li>
      <li>  Find the diversity combination with the largest number of false positives! </li>
    </p>

    <p>
      In order to prevent my headlines classifier and my headline generator from getting super clogged up with code
      for this part of the project, I decided it was time to refactor my code and make it more organized. So,
      I turned both classifier and generator into Python classes so that I could import them into a third program,
      called 'headline-gan.' Each instance of the classifier has access to its tokenizer and model so that I can
      make predictions, and each instance of the generator has access to its tokenizer and model so I can generate
      headlines in the headline-gan program. Now, all three of my programs are far more organized, split into
      multiple methods and both models and their methods are able to be used in other programs!
    </p>

    <p>
      Everything was in order for me to begin my GAN simulation, so I figured now was the time to begin.
    </p>

    <p>
      And once again, I figured wrong. Remember how I said overfitting came back to bite me in the butt?
      When I generated a thousand new headlines, and I fed them into the classifier, and I printed the
      false positives, I was beyond shocked to see that the classifier said there were around 75% false positives,
      meaning the model misclassified 750 out of the 1000 headlines... which is absolutely atrocious!
      But what all does this have to do with overfitting, you may ask? My theory is that the classifier
      performed so poorly on these headlines because it was trained on fake headlines generated with the same
      length (5 words) and only 8,000, in sets of 10, meaning there were only 800 unique starting words
      out of more than 11,000 possible words. When looking at the testing data, it's possible that the model
      performed so well because all the headlines looked similar enough to the training headlines, since they
      shared starting words and maintained the same length.
    </p>

    <p>
      Although I wasn't able to perform my experiment exactly how I wanted, I did learn quite a bit doing all of this.
      I realized that the training data needs to be larger and more diverse in order to prevent overfitting,
      so I'm going to completely retrain and rebuild my classifier using more real headlines and more fake ones.
      But, I'd also like to generate better fake headlines so that the model really has to rely on numerous
      features to decide the authenticity of a headline- this requires a redesign of my generator model
      as well. Refactoring my code was a necessary step that I'm glad I took now, so that was certainly one benefit of this experiment!
      I plan on improving my generator and my classifier so that I can attempt this experiment again,
      and maybe see some actual results. I'll keep this blog updated with what I find out!
    </p>

    <p> <a href= "page_5.html">Previous Page</a>
      <br/> <a href="page_7.html">Next Page</a>
    </p>

  </body>

</html>
